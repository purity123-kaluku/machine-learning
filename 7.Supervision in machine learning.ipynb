{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdc3f56-50ad-4a9a-b752-19e6cf1208d2",
   "metadata": {},
   "source": [
    "## supervision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdb8d06-4ed0-400c-a726-186259e8d0e2",
   "metadata": {},
   "source": [
    "Supervision in machine learning refers to the process by which models are trained on labeled data to make predictions or classifications. There are different types of supervision in machine learning, each with its own methodology, applications, and benefits.\n",
    "\n",
    "### Types of Supervision in Machine Learning\n",
    "\n",
    "1. **Supervised Learning**\n",
    "2. **Unsupervised Learning**\n",
    "3. **Semi-Supervised Learning**\n",
    "4. **Reinforcement Learning**\n",
    "\n",
    "### 1. Supervised Learning\n",
    "\n",
    "**Definition**: In supervised learning, the model is trained on a labeled dataset, meaning that each training example is paired with an output label. The model learns to map inputs to outputs based on these examples.\n",
    "\n",
    "**Characteristics**:\n",
    "- **Labeled Data**: Requires a dataset where each input is associated with a known output.\n",
    "- **Goal**: Learn a mapping from inputs to outputs to make predictions on new, unseen data.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "i)**Classification**:\n",
    "-  Assigning labels to inputs (e.g., spam detection in emails, image classification).\n",
    "-  Classification in supervised learning is a type of machine learning task where the goal is to assign input data to one of several predefined categories or classes. It involves training a model using a labeled dataset, where each input is paired with a corresponding class label, and then using the trained model to predict the class labels of new, unseen inputs.\n",
    "\n",
    "### Key Concepts in Classification\n",
    "\n",
    "1. **Labeled Data**: The training dataset consists of input-output pairs, where the output is a discrete class label. For example, in an email spam detection system, emails might be labeled as \"spam\" or \"not spam\".\n",
    "\n",
    "2. **Classes**: These are the distinct categories or labels to which the inputs can belong. For example, in a digit recognition system, the classes might be the digits 0 through 9.\n",
    "\n",
    "3. **Features**: The attributes or properties of the input data that are used by the model to make predictions. For example, in an email spam classifier, features might include the frequency of certain keywords, the length of the email, etc.\n",
    "\n",
    "4. **Model Training**: The process of learning the relationship between the input features and the class labels from the training data. This involves selecting an appropriate algorithm and optimizing its parameters.\n",
    "\n",
    "5. **Prediction**: The process of using the trained model to classify new inputs. The model assigns a class label to each new input based on what it has learned during training.\n",
    "\n",
    "### Common Algorithms for Classification\n",
    "\n",
    "1. **Logistic Regression**: Despite its name, logistic regression is used for binary classification tasks. It models the probability of the input belonging to a particular class using a logistic function.\n",
    "\n",
    "2. **Decision Trees**: These models use a tree-like structure of decisions based on the features of the input data to arrive at a class label.\n",
    "\n",
    "3. **Random Forest**: An ensemble method that builds multiple decision trees and combines their outputs to improve classification accuracy.\n",
    "\n",
    "4. **Support Vector Machines (SVM)**: These models find the hyperplane that best separates the classes in the feature space, maximizing the margin between the classes.\n",
    "\n",
    "5. **k-Nearest Neighbors (k-NN)**: A simple algorithm that classifies an input based on the majority class among its k nearest neighbors in the training data.\n",
    "\n",
    "6. **Neural Networks**: Complex models capable of learning intricate patterns in the data. Deep neural networks, in particular, have been successful in image and speech recognition tasks.\n",
    "\n",
    "### Example Code: Classification with Logistic Regression\n",
    "\n",
    "Here's a simple example of classification using logistic regression with the popular Iris dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred, target_names=iris.target_names)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\\n\", report)\n",
    "```\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "When evaluating the performance of a classification model, several metrics are commonly used:\n",
    "\n",
    "1. **Accuracy**: The proportion of correctly classified instances out of the total instances. \n",
    "   \\[\n",
    "   \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "   \\]\n",
    "\n",
    "2. **Precision**: The proportion of true positive predictions among all positive predictions made by the model.\n",
    "   \\[\n",
    "   \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
    "   \\]\n",
    "\n",
    "3. **Recall**: The proportion of true positive predictions among all actual positive instances.\n",
    "   \\[\n",
    "   \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
    "   \\]\n",
    "\n",
    "4. **F1 Score**: The harmonic mean of precision and recall, providing a balance between the two.\n",
    "   \\[\n",
    "   F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
    "   \\]\n",
    "\n",
    "5. **Confusion Matrix**: A table that shows the counts of true positive, true negative, false positive, and false negative predictions, giving a detailed breakdown of classification performance.\n",
    "\n",
    "\n",
    "Classification in supervised learning is a fundamental task that involves predicting the categorical label of input data. By leveraging labeled datasets and using various algorithms, classification models can be trained to accurately categorize new instances, making it a powerful tool in many practical applications, from spam detection to medical diagnosis and beyond.\n",
    "\n",
    "\n",
    "\n",
    "ii) **Regression**:\n",
    "\n",
    "-   Regression is another fundamental task in supervised learning, but unlike classification, which predicts discrete class labels, regression predicts continuous numerical values. The goal of regression analysis is to model the relationship between a dependent variable (often called the target or output) and one or more independent variables (often called features or inputs).\n",
    "\n",
    "### Key Concepts in Regression\n",
    "\n",
    "1. **Labeled Data**: The training dataset consists of input-output pairs, where the output is a continuous value. For example, predicting house prices based on features like square footage, number of bedrooms, and location.\n",
    "\n",
    "2. **Features**: The attributes or properties of the input data that are used by the model to make predictions. These can be numerical or categorical, but they are typically transformed into a suitable numerical form.\n",
    "\n",
    "3. **Model Training**: The process of learning the relationship between the input features and the continuous output from the training data. This involves selecting an appropriate algorithm and optimizing its parameters.\n",
    "\n",
    "4. **Prediction**: The process of using the trained model to predict the continuous value for new inputs.\n",
    "\n",
    "### Common Algorithms for Regression\n",
    "\n",
    "1. **Linear Regression**: Models the relationship between the dependent variable and one or more independent variables by fitting a linear equation to the observed data.\n",
    "   \\[\n",
    "   y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n\n",
    "   \\]\n",
    "\n",
    "2. **Polynomial Regression**: Extends linear regression by considering polynomial terms of the features, allowing for more complex relationships.\n",
    "   \\[\n",
    "   y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_n x^n\n",
    "   \\]\n",
    "\n",
    "3. **Ridge and Lasso Regression**: Regularized versions of linear regression that add a penalty for large coefficients to prevent overfitting.\n",
    "   - **Ridge Regression**: Adds an \\(L2\\) penalty.\n",
    "   - **Lasso Regression**: Adds an \\(L1\\) penalty.\n",
    "\n",
    "4. **Decision Trees and Random Forests**: Tree-based methods that partition the data into subsets based on feature values and fit simple models within each partition.\n",
    "   - **Decision Trees**: Splits the data into segments recursively.\n",
    "   - **Random Forests**: An ensemble of decision trees that improves performance by averaging the predictions.\n",
    "\n",
    "5. **Support Vector Regression (SVR)**: Uses the principles of support vector machines for regression tasks, fitting the data within a margin of tolerance.\n",
    "\n",
    "6. **Neural Networks**: Can model complex relationships in data through multiple layers of interconnected neurons. Deep learning techniques are particularly effective for high-dimensional data.\n",
    "\n",
    "### Example Code: Regression with Linear Regression\n",
    "\n",
    "Here's a simple example of regression using linear regression with the Boston Housing dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "```\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "When evaluating the performance of a regression model, several metrics are commonly used:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: The average of the squared differences between the predicted and actual values. It penalizes larger errors more heavily.\n",
    "   \\[\n",
    "   \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "   \\]\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**: The square root of MSE, providing an error metric in the same units as the target variable.\n",
    "   \\[\n",
    "   \\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "   \\]\n",
    "\n",
    "3. **Mean Absolute Error (MAE)**: The average of the absolute differences between the predicted and actual values. It provides a linear measure of the error.\n",
    "   \\[\n",
    "   \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "   \\]\n",
    "\n",
    "4. **R-squared (\\(R^2\\))**: A statistical measure of how well the regression model fits the data. It represents the proportion of the variance in the dependent variable that is predictable from the independent variables.\n",
    "   \\[\n",
    "   R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\n",
    "   \\]\n",
    "\n",
    "\n",
    "\n",
    "Regression in supervised learning is essential for predicting continuous outcomes based on input features. It encompasses a variety of algorithms, from simple linear regression to complex neural networks, each suited for different types of data and relationships. Properly evaluating regression models using metrics like MSE, RMSE, MAE, and \\(R^2\\) ensures the model's accuracy and reliability in making predictions.\n",
    "\n",
    "**Example Code**:\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example dataset\n",
    "X = [[1], [2], [3], [4], [5]]\n",
    "y = [1, 4, 9, 16, 25]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "```\n",
    "\n",
    "### 2. Unsupervised Learning\n",
    "\n",
    "**Definition**: Unsupervised learning involves training a model on data without labeled responses. The goal is to uncover hidden patterns or intrinsic structures in the input data.\n",
    "\n",
    "**Characteristics**:\n",
    "- **Unlabeled Data**: Works with datasets that have no associated output labels.\n",
    "- **Goal**: Discover underlying patterns or groupings in the data.\n",
    "\n",
    "**Examples**:\n",
    "\n",
    "i) **Clustering**: \n",
    "Grouping similar data points together (e.g., customer segmentation, image segmentation).\n",
    "Clustering in unsupervised learning is a type of machine learning task that involves grouping a set of objects or data points into clusters, where objects in the same cluster are more similar to each other than to those in other clusters. This method is useful for discovering the inherent structure or patterns in the data without predefined labels.\n",
    "\n",
    "### Key Concepts in Clustering\n",
    "\n",
    "1. **Unlabeled Data**: Clustering algorithms work on datasets that do not have labeled responses. The goal is to find natural groupings within the data.\n",
    "\n",
    "2. **Clusters**: These are groups of data points that are similar to each other based on certain criteria, such as distance or density.\n",
    "\n",
    "3. **Similarity Measure**: A metric used to determine how similar or dissimilar data points are. Common measures include Euclidean distance, Manhattan distance, and cosine similarity.\n",
    "\n",
    "### Common Clustering Algorithms\n",
    "\n",
    "1. **K-Means Clustering**: \n",
    "   - **Description**: Partitions the data into \\(k\\) clusters by minimizing the sum of squared distances between data points and the centroid of the cluster to which they belong.\n",
    "   - **Steps**:\n",
    "     1. Initialize \\(k\\) centroids randomly.\n",
    "     2. Assign each data point to the nearest centroid.\n",
    "     3. Recompute the centroids as the mean of the assigned data points.\n",
    "     4. Repeat steps 2 and 3 until convergence.\n",
    "\n",
    "2. **Hierarchical Clustering**:\n",
    "   - **Description**: Builds a tree-like structure (dendrogram) of clusters by recursively merging or splitting clusters.\n",
    "   - **Types**:\n",
    "     - **Agglomerative**: Starts with each data point as a separate cluster and merges the closest pairs until one cluster remains.\n",
    "     - **Divisive**: Starts with all data points in one cluster and recursively splits them.\n",
    "\n",
    "3. **DBSCAN (Density-Based Spatial Clustering of Applications with Noise)**:\n",
    "   - **Description**: Groups data points that are closely packed together, marking points in low-density regions as outliers.\n",
    "   - **Advantages**: Can find arbitrarily shaped clusters and handle noise (outliers).\n",
    "\n",
    "4. **Gaussian Mixture Models (GMM)**:\n",
    "   - **Description**: Assumes that the data is generated from a mixture of several Gaussian distributions with unknown parameters.\n",
    "   - **Method**: Uses the Expectation-Maximization (EM) algorithm to find the parameters of the Gaussian distributions.\n",
    "\n",
    "### Example Code: K-Means Clustering\n",
    "\n",
    "Here's a simple example of clustering using K-Means with the Iris dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Perform K-Means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get cluster labels\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the dataset for visualization\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# Plot the clusters\n",
    "plt.scatter(df.iloc[:, 0], df.iloc[:, 1], c=df['Cluster'], cmap='viridis', marker='o')\n",
    "plt.title('K-Means Clustering of Iris Dataset')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "Evaluating clustering results can be challenging since there are no predefined labels. However, several metrics are commonly used:\n",
    "\n",
    "1. **Silhouette Score**: Measures how similar a data point is to its own cluster compared to other clusters. Values range from -1 to 1, with higher values indicating better clustering.\n",
    "   \\[\n",
    "   \\text{Silhouette Score} = \\frac{b - a}{\\max(a, b)}\n",
    "   \\]\n",
    "   where \\(a\\) is the mean distance between a sample and all other points in the same cluster, and \\(b\\) is the mean distance between a sample and all points in the next nearest cluster.\n",
    "\n",
    "2. **Davies-Bouldin Index**: Measures the average similarity ratio of each cluster with the cluster most similar to it. Lower values indicate better clustering.\n",
    "   \\[\n",
    "   \\text{DB Index} = \\frac{1}{n} \\sum_{i=1}^{n} \\max_{j \\neq i} \\left( \\frac{s_i + s_j}{d_{ij}} \\right)\n",
    "   \\]\n",
    "   where \\(s_i\\) is the average distance between each point in cluster \\(i\\) and the centroid of \\(i\\), and \\(d_{ij}\\) is the distance between centroids of clusters \\(i\\) and \\(j\\).\n",
    "\n",
    "3. **Inertia (Within-Cluster Sum of Squares)**: Measures the sum of squared distances between each point and its assigned cluster centroid. Lower values indicate more compact clusters.\n",
    "\n",
    "### Applications of Clustering\n",
    "\n",
    "- **Customer Segmentation**: Grouping customers based on purchasing behavior or demographics for targeted marketing.\n",
    "- **Anomaly Detection**: Identifying unusual patterns or outliers in data, such as fraud detection in financial transactions.\n",
    "- **Document Classification**: Grouping similar documents together for topic modeling or information retrieval.\n",
    "- **Image Segmentation**: Partitioning images into meaningful regions for object detection or image analysis.\n",
    "- **Genomic Data Analysis**: Grouping genes or proteins with similar expression patterns for biological studies.\n",
    "\n",
    "\n",
    "Clustering in unsupervised learning is a powerful tool for discovering hidden patterns and structures in data. It involves grouping data points into clusters based on similarity, with various algorithms available to handle different types of data and cluster shapes. Evaluating clustering results can be challenging, but metrics like silhouette score and Davies-Bouldin index help assess the quality of the clusters. Clustering has diverse applications across many fields, making it an essential technique in data analysis and machine learning.\n",
    "\n",
    "ii) **Dimensionality Reduction**:\n",
    "\n",
    "Reducing the number of features while preserving important information (e.g., PCA, t-SNE).\n",
    "\n",
    "Dimensionality reduction is a crucial technique in unsupervised learning used to reduce the number of features or dimensions in a dataset while preserving as much relevant information as possible. This is important for simplifying models, visualizing high-dimensional data, and improving computational efficiency. There are various methods for dimensionality reduction, each with its own approach and advantages.\n",
    "\n",
    "### Key Concepts in Dimensionality Reduction\n",
    "\n",
    "1. **High-Dimensional Data**: Datasets with a large number of features, which can be challenging to analyze and visualize.\n",
    "\n",
    "2. **Curse of Dimensionality**: As the number of dimensions increases, the volume of the feature space grows exponentially, making data sparse and algorithms less effective.\n",
    "\n",
    "3. **Projection**: Mapping high-dimensional data to a lower-dimensional space.\n",
    "\n",
    "4. **Feature Selection vs. Feature Extraction**:\n",
    "   - **Feature Selection**: Selecting a subset of the original features.\n",
    "   - **Feature Extraction**: Creating new features by combining the original features.\n",
    "\n",
    "### Common Dimensionality Reduction Techniques\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**:\n",
    "   - **Description**: PCA transforms the data into a new coordinate system where the greatest variance comes to lie on the first coordinate (principal component), the second greatest variance on the second coordinate, and so on.\n",
    "   - **Method**: It uses eigenvalue decomposition of the covariance matrix or singular value decomposition (SVD) to identify the principal components.\n",
    "   - **Applications**: Data visualization, noise reduction, and as a preprocessing step for other machine learning algorithms.\n",
    "\n",
    "2. **t-Distributed Stochastic Neighbor Embedding (t-SNE)**:\n",
    "   - **Description**: t-SNE is a non-linear dimensionality reduction technique that is particularly well-suited for embedding high-dimensional data into a two- or three-dimensional space for visualization.\n",
    "   - **Method**: It minimizes the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points.\n",
    "   - **Applications**: Visualizing high-dimensional data, such as in genomics and image data.\n",
    "\n",
    "3. **Linear Discriminant Analysis (LDA)**:\n",
    "   - **Description**: LDA is a supervised method that finds the linear combinations of features that best separate two or more classes.\n",
    "   - **Method**: It projects the data in a way that maximizes the ratio of between-class variance to within-class variance.\n",
    "   - **Applications**: Classification tasks, where reducing dimensionality can improve model performance.\n",
    "\n",
    "4. **Independent Component Analysis (ICA)**:\n",
    "   - **Description**: ICA separates a multivariate signal into additive, independent components. It is a generalization of PCA but seeks to find statistically independent components.\n",
    "   - **Method**: It maximizes statistical independence among the components.\n",
    "   - **Applications**: Signal processing, such as separating mixed audio signals or sources.\n",
    "\n",
    "5. **Autoencoders**:\n",
    "   - **Description**: Autoencoders are a type of artificial neural network used to learn efficient codings of input data. They consist of an encoder that compresses the data and a decoder that reconstructs it.\n",
    "   - **Method**: The network is trained to minimize the reconstruction error between the input and the reconstructed output.\n",
    "   - **Applications**: Image compression, anomaly detection, and as a preprocessing step for deep learning.\n",
    "\n",
    "### Example Code: Dimensionality Reduction with PCA\n",
    "\n",
    "Here's a simple example of using PCA for dimensionality reduction on the Iris dataset:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the PCA-transformed data\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.colorbar(scatter, label='Target Label')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Applications of Dimensionality Reduction\n",
    "\n",
    "1. **Data Visualization**: Reducing high-dimensional data to two or three dimensions for visualization.\n",
    "2. **Preprocessing**: Simplifying the dataset to improve the performance and training time of machine learning models.\n",
    "3. **Noise Reduction**: Removing noise from data by keeping only the most informative components.\n",
    "4. **Feature Extraction**: Creating new features that capture the most variance in the data.\n",
    "5. **Compression**: Reducing the storage space needed for data by compressing it into fewer dimensions.\n",
    "\n",
    "\n",
    "Dimensionality reduction is a vital technique in unsupervised learning for handling high-dimensional data. Methods like PCA, t-SNE, LDA, ICA, and autoencoders each have unique strengths and applications, from data visualization to improving machine learning model performance. Proper application of these techniques can lead to more efficient and insightful data analysis.\n",
    "\n",
    "\n",
    "**Example Code**:\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [4, 2], [4, 4], [4, 0]])\n",
    "\n",
    "# Train the model\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Predict cluster labels\n",
    "labels = kmeans.labels_\n",
    "print(f\"Cluster Labels: {labels}\")\n",
    "```\n",
    "\n",
    "### 3. Semi-Supervised Learning\n",
    "\n",
    "**Definition**: Semi-supervised learning uses both labeled and unlabeled data to train models. It is useful when acquiring labeled data is expensive or time-consuming, but there is an abundance of unlabeled data.\n",
    "\n",
    "**Characteristics**:\n",
    "- **Combination of Labeled and Unlabeled Data**: Leverages a small amount of labeled data with a larger pool of unlabeled data.\n",
    "- **Goal**: Improve learning performance by making use of all available data.\n",
    "\n",
    "**Examples**:\n",
    "- **Transductive Learning**: Training with both labeled and unlabeled data and making predictions only on the unlabeled portion used during training.\n",
    "- **Inductive Learning**: Generalizing from both labeled and unlabeled data to new unseen data.\n",
    "\n",
    "**Example Code** (using label propagation):\n",
    "```python\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [4, 4], [4, 0]])\n",
    "y = np.array([0, 0, 0, -1, -1, -1])  # -1 indicates unlabeled data\n",
    "\n",
    "# Train the model\n",
    "label_prop = LabelPropagation()\n",
    "label_prop.fit(X, y)\n",
    "\n",
    "# Predict labels for unlabeled data\n",
    "labels = label_prop.transduction_\n",
    "print(f\"Labels: {labels}\")\n",
    "```\n",
    "\n",
    "### 4. Reinforcement Learning\n",
    "\n",
    "**Definition**: Reinforcement learning involves training agents to make sequences of decisions by rewarding desired behaviors and/or punishing undesired ones. The agent learns to achieve a goal in an uncertain, potentially complex environment.\n",
    "\n",
    "**Characteristics**:\n",
    "- **Interaction with Environment**: The agent interacts with the environment to learn from the consequences of its actions.\n",
    "- **Reward System**: Uses rewards or penalties as feedback to reinforce learning.\n",
    "\n",
    "**Examples**:\n",
    "- **Game Playing**: Training models to play games like Chess, Go, or video games (e.g., AlphaGo, DQN).\n",
    "- **Robotics**: Teaching robots to perform tasks like walking, picking objects, or navigation.\n",
    "\n",
    "**Example Code** (using a simple Q-learning algorithm):\n",
    "```python\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Create the environment\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# Initialize Q-table\n",
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "num_episodes = 1000\n",
    "\n",
    "# Training the agent\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q[state, :])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        state = next_state\n",
    "\n",
    "# Display trained Q-table\n",
    "print(\"Trained Q-table:\")\n",
    "print(Q)\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Supervision in machine learning defines the way models learn from data. Supervised and unsupervised learning are the most commonly used paradigms, while semi-supervised learning offers a compromise between the two, and reinforcement learning is distinct in its approach of learning through interaction and feedback. Each type of supervision has its specific applications and is chosen based on the nature of the problem and the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2bee60-47cf-4cdf-af50-8892b8e4cfc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
