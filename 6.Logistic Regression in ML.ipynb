{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86f57f5-937f-4e86-88a0-4e6071ff339a",
   "metadata": {},
   "source": [
    "## Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7d40f3-7913-4eba-8320-1a6d25b4a07f",
   "metadata": {},
   "source": [
    "Logistic Regression is a statistical method used for binary classification problems, where the outcome can take one of two possible values (e.g., yes/no, true/false, 0/1). It models the probability that a given input belongs to a particular class.\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "#### Logistic Function (Sigmoid Function)\n",
    "The logistic regression model uses the logistic function to map predicted values to probabilities. The logistic function is defined as:\n",
    "\n",
    "\\[ \\sigma(z) = \\frac{1}{1 + e^{-z}} \\]\n",
    "\n",
    "where \\( z \\) is the input to the function, which is a linear combination of the input features.\n",
    "\n",
    "#### Model Representation\n",
    "The hypothesis of logistic regression is given by:\n",
    "\n",
    "\\[ h_\\theta(x) = \\sigma(\\theta^T x) = \\frac{1}{1 + e^{-\\theta^T x}} \\]\n",
    "\n",
    "where \\( \\theta \\) is the vector of parameters (weights) and \\( x \\) is the vector of input features.\n",
    "\n",
    "#### Cost Function\n",
    "The cost function for logistic regression is the logistic loss (also known as log loss or cross-entropy loss):\n",
    "\n",
    "\\[ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m \\left[ y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) \\right] \\]\n",
    "\n",
    "where \\( m \\) is the number of training examples, \\( y^{(i)} \\) is the true label, and \\( h_\\theta(x^{(i)})) \\) is the predicted probability for the \\(i\\)-th example.\n",
    "\n",
    "#### Training the Model\n",
    "The model is trained using optimization algorithms like gradient descent to minimize the cost function and find the optimal parameters \\( \\theta \\).\n",
    "\n",
    "### Example Code Using Python and Scikit-Learn\n",
    "\n",
    "Here's an example of how to implement logistic regression using Python's Scikit-Learn library:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load a dataset (e.g., iris dataset for simplicity)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
    "y = (iris.target != 0) * 1  # Convert to a binary classification problem (e.g., class 0 vs. not class 0)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot decision boundary\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "plt.xlabel(iris.feature_names[0])\n",
    "plt.ylabel(iris.feature_names[1])\n",
    "plt.title('Logistic Regression Decision Boundary')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Key Points to Remember\n",
    "- **Binary Classification**: Logistic regression is primarily used for binary classification problems.\n",
    "- **Probability Output**: The model outputs probabilities that are mapped to class labels using a threshold (e.g., 0.5).\n",
    "- **Cost Function**: The log loss function is used to measure the performance of the model.\n",
    "- **Interpretability**: The coefficients \\( \\theta \\) in the logistic regression model can be interpreted to understand the influence of each feature on the probability of the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7da1afa-4951-438f-834b-5982c7e398e9",
   "metadata": {},
   "source": [
    "## Coefficient\n",
    "- In logistic regression the coefficient is the expected change in log-odds of having the outcome per unit change in X.\n",
    "- This does not have the most intuitive understanding so let's use it to create something that makes more sense, odds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9093bf85-ed75-4ae8-b7d2-6b29f8fa575b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.03557295]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import linear_model\n",
    "\n",
    "#Reshaped for Logistic function.\n",
    "X = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n",
    "y = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "logr = linear_model.LogisticRegression()\n",
    "logr.fit(X,y)\n",
    "\n",
    "log_odds = logr.coef_\n",
    "odds = numpy.exp(log_odds)\n",
    "\n",
    "print(odds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b17aa-9066-4add-90a9-31dd8f7b5e24",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80846ba5-a9f3-47d4-808c-af13ed75aa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.60749168]\n",
      " [0.19267555]\n",
      " [0.12774788]\n",
      " [0.00955056]\n",
      " [0.08037781]\n",
      " [0.0734485 ]\n",
      " [0.88362857]\n",
      " [0.77901203]\n",
      " [0.88924534]\n",
      " [0.81293431]\n",
      " [0.57718238]\n",
      " [0.96664398]]\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import linear_model\n",
    "\n",
    "X = numpy.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n",
    "y = numpy.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "logr = linear_model.LogisticRegression()\n",
    "logr.fit(X,y)\n",
    "\n",
    "def logit2prob(logr, X):\n",
    "  log_odds = logr.coef_ * X + logr.intercept_\n",
    "  odds = numpy.exp(log_odds)\n",
    "  probability = odds / (1 + odds)\n",
    "  return(probability)\n",
    "\n",
    "print(logit2prob(logr, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e518cd9-c58a-40e6-86c6-23e7bd2e05c5",
   "metadata": {},
   "source": [
    "3.78 0.61 The probability that a tumor with the size 3.78cm is cancerous is 61%.\n",
    "\n",
    "2.44 0.19 The probability that a tumor with the size 2.44cm is cancerous is 19%.\n",
    "\n",
    "2.09 0.13 The probability that a tumor with the size 2.09cm is cancerous is 13%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59b16ac-4841-49df-a60b-ded68f660150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
